<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>机器学习笔记-KNN（统计学习方法-第3章） | Travis, Xu</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/about">个人介绍</a></li>
      
      <li><a href="/note">随笔笔记</a></li>
      
      <li><a href="/books">个人书架</a></li>
      
      <li><a href="/links">关注链接</a></li>
      
    </ul>
    <hr/>
    </nav>




<div class="article-meta">
  <h1><span class="title">机器学习笔记-KNN（统计学习方法-第3章）</span></h1>
  
  <h2 class="date">2020-10-07</h2>
</div>



<main>
  

<p>最近在疏理《统计学习方法》中的知识点，方便自己后续复习。这里记录下第3章K近邻算法的基本思想。</p>

<p>K近邻（KNN）是一种基本分类与回归方法。给定目标样本通过K个邻居来决定目标样本的标签。K近邻法有三个基本要素：K值的选择、距离度量和分类回归决策规则。KNN并不需要训练，但需要遍历整个训练集，所以预测比较慢，书中提到用KD树进行优化，来提高K近邻搜索的效率。</p>

<h2 id="k近邻基本要素">K近邻基本要素</h2>

<h3 id="距离度量">距离度量</h3>

<p>KNN一般采用欧式距离，但更一般的是 <code>$L_p$</code> 距离， <code>$L_p$</code> 距离在机器学习中经常会看到。</p>

<p>设特征空间 <code>$\chi$</code> 是 <code>$n$</code> 维实数向量空间 <code>$R^n$</code> ， <code>$x_i, x_j \in \chi$</code> ， <code>$x_i = (x_i^1, x_i^2, ..., x_i^n)^T$</code> ， <code>$x_j = (x_j^1, x_j^2, ..., x_j^n)^T$</code> ， <code>$x_i, x_j$</code> 的 <code>$L_p$</code> 距离定义为
<code>$L_p(x_i, x_j) = (\sum_{l=1}^n |x_i^l - x_j^l|^p) ^{\frac{1}{p}}$</code> ，其中 <code>$p \geqslant 1$</code></p>

<p>当 <code>$p = 1$</code> 时，称为曼哈顿距离，即 <code>$L_1(x_i, x_j) = \sum_{l=1}^n |x_i^l - x_j^l|$</code> ；</p>

<p>当 <code>$p = 2$</code> 时，称为欧式距离，即 <code>$L_2(x_i, x_j) = (\sum_{l=1}^n |x_i^l - x_j^l|^2) ^{\frac{1}{2}}$</code> ，写成我们最熟悉的形式 <code>$\sqrt{(\Delta x_1)^2 + (\Delta x_2)^2 + ... + (\Delta x_n)^2}$</code> ；</p>

<p>当 <code>$p = \infty$</code> ，它是各个坐标距离的最大值，即 <code>$L_\infty(x_i, x_j) = \underset{l}{max} \ |x_i^l - x_j^l|$</code> 。</p>

<p>下图给出二维空间中 <code>$p$</code> 取不同值时，与原点的 <code>$L_p$</code> 距离为1（ <code>$L_p = 1$</code> ）的点的图形。</p>

<div align="center"> <img src="/images/20201007_knn.png" alt="avatar" style="zoom:50%;" /> </div>

<p><strong>由上图，我们可以从直观看出 <code>$L_p$</code> 当 <code>$p$</code> 取不同值时的一些性质，当 <code>$p=2$</code> 时即欧式距离（也称L2范式），相比平滑很多，而且可导。正因为它的平滑性和可导性，在机器习学中大量使用 L2 范式：作为损失函数（又名最小二乘法误差，即least squares error, LSE）、作为正则项（即 <code>$\lambda \sum_{i=1}^k (w_i)^2$</code> ）。</strong></p>

<h3 id="k值选择和决策">K值选择和决策</h3>

<p>K值选择会对KNN的结果产生很大影响。K既不能太小也不能太大。如果选择较小的 <code>$K$</code> 值，极端取 <code>$K=1$</code> 就意味着模型变得复杂，容易发生过拟合；如果选择较大的 <code>$K$</code> 值，极端取 <code>$K = N$</code> （ <code>$N$</code> 即为样本的数量），那么模型过于简单（容易欠拟合）。书上用了近似误差（关注的是训练集）和估计误差（关注的是测试集即泛化性）来描述的，知乎上有关于近似误差和估计误差的详细解释（<a href="https://www.zhihu.com/question/60793482/answer/1044887227">链接</a>）。</p>

<p><strong>在实际应用中， <code>$K$</code> 值先取一个较小的数值，再逐渐增大。可采用交叉验证来选取最优的 <code>$K$</code> 值。</strong></p>

<p>KNN在分类问题中往往是多数表决，即由输入实例的K个邻近的训练实例中的多数类决定最终结果；</p>

<p>KNN在回归问题中是取平均，即对样本的K个邻近标签值取平均，作为预测结果。</p>

<h2 id="kd树">KD树</h2>

<p>K近邻算法如果用线性扫描，则时间复杂度为 O(N)，当训练集很大时，计算非常耗时；所以为了提高K近邻搜索效率，使用特殊的存储结构（即KD树），以减少扫描的次数，时间复杂度为 O(logN)；<strong>可以理解为用空间换时间，空间体现在存储结构的构建，这种“空间换时间”的思想在机算机科学里屡见不鲜。</strong></p>

<p>KD树需要 <strong>先构造，再搜索</strong>，注意这里的 “KD树的K” 和 “K近邻的K” 含义不同，这里的K是指数据的维度。</p>

<h3 id="kd树构造">KD树构造</h3>

<p>输入：样本集 <code>$X = \{x_1, x_2, x_3, ..., x_n\}$</code> ，其中 <code>$x_i = \{x_i^1, x_i^2, ..., x_i^m\}$</code> ， <code>$m$</code> 是指特征维度（和KD树中的K是相同的含义），为了和K近邻的K区分开，这里用m代替。</p>

<p>构造根结点，即首次 <code>$m=1$</code> ，根据第一维特征的大小将所有样本排序，以中位数为中心将样本分为两类，分别作为根节点的左右子树；</p>

<p>对于左右子树，分别选取 <code>$m=\{2,3,...,m\}$</code> 维特征按照中位数进行子树的划分；若叶子结点有多个样本，重复选取 <code>$m=\{1,2,...,m\}$</code> 再切分样本空间，直至每个叶子结点仅有一个样本为止。</p>

<p><strong>注：假设深度为 <code>$j$</code> ，则结点选择维度为 <code>$l=(j\%m + 1)$</code> 的中位数进行切分，取模是个循环重复的过程，直至每个叶子仅有一个样本。</strong></p>

<h3 id="kd树搜索">KD树搜索</h3>

<p>从根结点 <strong>自顶向下</strong> 基于树的二分查找找到到叶子结点；从找到的叶子结点 <strong>自底向上</strong> 找出K个最相邻的样本，自底向上查找的过程可参考下《统计学习方法》书上的例子（直接看算法描述有点晦涩难懂）。
整个时间复杂度是 <code>$2 \times O(logN)$</code> （自顶向下 和 自底向上 均为 O(logN)），最以最终时间复杂度为 O(logN)。</p>

<h2 id="总结">总结</h2>

<p>这里总结了下KNN算法，它的三个基本要素（即距离度量、K值选择和决策），以及KNN的一个经典实现（KD树）。后续学习下KNN的的另一个实现（Ball树），它和KD树的问题出发点相似，都是用空间换时间来解决时间复杂度问题。</p>

<p>KNN的理论比较通俗易懂，后续有空可以尝试些代码的实践，KNN的包在sklearn里有集成（ <a href="https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification">链接</a> ）。</p>

<h2 id="参考文献">参考文献</h2>

<ol>
<li><a href="https://www.zhihu.com/question/26485586/answer/616029832">l1正则与l2正则的特点是什么，各有什么优势？</a> ，知乎</li>
<li>《统计学习方法》第3章-K近邻法，李航</li>
<li><a href="https://blog.csdn.net/lekusun9671/article/details/104716046">统计学习方法-K近邻算法</a> ，CSDN</li>
</ol>

</main>



  <footer>
  <script src="//yihui.name/js/math-code.js"></script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



  
  </footer>
  </body>
</html>

