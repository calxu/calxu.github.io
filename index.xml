<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Travis, Xu</title>
    <link>https://calxu.github.io/</link>
    <description>Recent content on Travis, Xu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 01 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://calxu.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>利用开源frp&#43;ss升级搭建翻墙VPN</title>
      <link>https://calxu.github.io/note/20200301_vpn_2/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/note/20200301_vpn_2/</guid>
      <description>之前直接利用ss搭建翻墙VPN已经成功被 GFW 封杀，可能是因为是建墙者可以成功检测出 利用 ss 传输协议的特征，从而利用传输协议的特征来封禁服务端的IP，最终导致我 ping 都不能ping通 服务端的机器。很是无语，最终尝试了不同地域的境外的 VPS 云服务，都以 IP 被封禁而告终。
最近朋友跟我说可以利用frp中转ss的方法避开 GFW 的检测，于是我 Google 了下关键字进行学习和尝试，用了一天时间折腾终于搭建成功了。目前该方法的流量协议未被 GFW 检测到，但不代表该方法会一直有效下去，因为GFW 流量检测协议的策略本身就是个不断升级的工作，所以我们也要不断的升级翻墙VPN的方法，这也是市面上很多卖翻墙VPN的工具帐号不能永久有效的原因。
下图给出frp+ss升级搭建中转翻墙VPN的概要原理图，由下图可知主要包含三部分的配置：客户端ss、中转服务器和ss服务器。这篇笔记首先介绍客户端ss的配置，作为用户你只需要配置客户端ss即可，其余无需关心；然后介绍中转服务器的配置；其次介绍ss服务器端的配置；最后总结并对未来针对 GFW 作一些未来应对策略。
客户端ss配置 PC客户端软件下载可到 GitHub 发布的 Releases 下载最新的版本即可（ windows版 和 Mac版 ），移动端（Android版），苹果移动端也是有的可以自己搜索下。
客户端ss大致的工作原理可以参考下以前写的 blog 。PC客户端可以结合Chrome插件 SwitchyOmega 使用，其中ss信息配置如下（不要薅我流量，黑户也不要攻击我的机器，仅供程序员学习使用）：
地 址： 39.97.122.11:12345 加密方法： aes-256-cfb 密 码： Dont_be_evil 备 注： Dont_be_evil  ss配置完可以用百度看下IP地址，IP地址重定向到香港的阿里云即配置正确，如下图所示方式查看测试结果：
中转服务器配置 中转服务器我是选用的是北京阿里云，成本有点高（大概60元/月，唉，只有每个月省点了）。中转服务器主要负责将请求转发到ss服务器，然后由ss服务器完成并返回至中转服务器，最后中转服务器将数据返给客户端。中转服务器的配置主要是 frps 服务端的配置，它负责请求的转发。下面就环境安装、服务端配置和启动服务作一下说明。
环境安装： frp软件可到Github 的releases 的 官方网站 进行下载解压即可。
服务端配置： 修改以上解压后目录的 frps.ini 文件，详细如下：
[common] # 穿透服务器端监听的端口，默认7000 bind_port = 7000 # 管理日志：日志存放位置；日志记录级别（debug/info/warn/error）；保存天数 log_file = .</description>
    </item>
    
    <item>
      <title>工程篇-图谱工具Neo4j搭建与入门学习</title>
      <link>https://calxu.github.io/note/20200226_neo4j_1/</link>
      <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/note/20200226_neo4j_1/</guid>
      <description>图谱近几年在互联网公司里用得非常普遍。据我所知，在 推荐领域 和 风控反作弊领域 用得非常多，最近也面试了数据挖掘岗的大大小小的互联网公司，大概 30% 的公司都问了关于图谱的一些知识。所以关于图谱的应用的需求还是非常大的，还是有必要学习下图谱的工业界一些通用的工具和套路。
图谱在应用中有一个很大的优点，它的解释能力很强，它基于一个“人以类聚，物以群分“原则，比如在社交图谱中，一个用户的所有的朋友学历水平能非常好地反映这个用户学历水平。所以在很多项目中，很多时候都是以用户近邻的特征来对该用户进行特征刻画的。
基本图的存储在大学里的《数据结构》中描述了最基本存储结构有 邻接表 和 邻接矩阵 。而对于图谱的存储，会比基本图的存储更加复杂，图谱存储细节后期再慢慢探索。好在工业界有一些非常成熟的图谱数据库的框架，目前比较流行的有 neo4j 和 orientdb 。
关于技术框架的选型， 我一般从两个层面来进行比较：1. 目前工业界普遍使用哪种，一方面身边朋友的部门在技术选型的时候用 neo4j 居多，另一方面在 数据库排行 存储图的DB中，neo4j 工业界普及率排第一，用得人多相对而言遇到各种问题，网上都可以找到答案，用得人少则会有很多隐藏着的坑；2.比较社区的活跃度，在Github活跃度 neo4j 和 orientdb ，前者活跃度 综合来看 要好于后者。综上两大点，我也选择 neo4j 进行学习。
这篇笔记主要学习下Neo4j环境的搭建，搭建包含服务端配置和客户端使用；然后再介绍下Neo4j入门级的非常常用的语法；最后进行了一个小的总结。
Neo4j环境搭建 服务端配置  服务端的选择：我一般习惯Linux系统进行后台配置，不太习惯Mac OS（兼容性会略有差别）。所以一般习惯把服务挂在Linux系统的云端，目前 云 已经渐渐成为一个基础设施了，如果只是做一些小的测试型的实验，一般最低配云服务器就可以了，价格也很便宜。我一般习惯 阿里云/腾讯云/百度云 轮流薅一遍它们的活动价，今晚薅到一台最低配的百度云 84元/年，够自己折腾一年了 ^_^。
 服务端的环境依赖：对于环境依赖，没有去深究细节的依赖关系。只是按照 Github 上 Dependencies 部分挨个安装了下，Ubuntu操作系统操作细节如下：
apt update apt install maven openjdk-8-jdk apt install debhelper devscripts dos2unix dpkg make xmlstarlet curl -O http://download-keycdn.ej-technologies.com/install4j/install4j_linux_6_1_4.deb dpkg -i install4j_linux_6_1_4.</description>
    </item>
    
    <item>
      <title>数据挖掘笔记-时序特征的加工</title>
      <link>https://calxu.github.io/note/20200212_timeseries_features/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/note/20200212_timeseries_features/</guid>
      <description>在工业界，不管是做风控模型还是做推荐系统，都会涉及到特征工程，特征工程往往直接决定着最终模型的效果。 在特征工程中如何来表征出数据中的信息，结合特定的场景是有一定的技巧和套路的方法论的。 记得以前在学校里打Kaggle比赛，有选手就专门总结出做特征的模版，不管什么挖掘类的比赛直接拿过来套就可以了，这很大程度加快了模型调测的速度。
在特征中有一类非常重要特征是时序特征。 时序信息的表征总得来说有两大方法： 1. 第一种方法是通过特征工程的方式来加工时序特征，这种方法的优点 可解释性强；缺点 很容易信息表达不全，严重依赖于对场景的经验的理解； 2. 第二种方法是通过LSTM时序神经网络的方法来学得时序信息，这种方法优点 信息表达全，能学得内在潜在规律；缺点 可解释性差，需要调网络参数。
以上两种方法各有优缺点。 我是从事金融行业的，很多时候需要追求可解释性和操作的方便性，所以有不少场景是通过特征工程的方式来加工时序特征的。 这里主要介绍下特征工程的方式来加工时序特征。 当时在学校师兄在微软实习时，微软根据Office产品使用情况来加工相应时序特征进行预测用户的付款意愿。 回学校后我们对它进行了总结归纳，时序特征的加工如下（以下是直接用LaTeX生成的PDF文件显示的，等有空时我把它处理成Markdown格式的，源文件可到这里下载 PDF文件 和 LaTeX源文件 ）。</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://calxu.github.io/about/</link>
      <pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/about/</guid>
      <description>个人简介 我叫 徐彩旭（英文名 Travis,Xu），性别男，93年生。 目前在 京东 从事 数据挖掘的工作，涉及主要业务是 企业风险方向。 我先后在 百度 实习，后转正留用 度小满金融（前百度金融） 从事 数据挖掘的工作，涉及主要业务是 个人风险的风控。 之前，我在 苏州大学 和 东南大学成贤学院 分别获得我的 硕士 和 学士 学位，均为CS专业。 这里附一份我的 中文简历（2020年02月更新）， 欢迎一起交流学习，我的邮箱：csxucaixu@gmail.com。
个人相关链接： Linkedin Github
个人兴趣： 爱代码，爱折腾，爱数学，爱篮球，爱生活。
自评： 做技术的手艺人。
近期工作  目前从事企业风控图谱的应用开发；
 每天坚持学习英语；
  工作背景  2020.02-至今： &amp;nbsp; 京东金融，北京大兴区，数据挖掘工程师。
 2018.07-2020.01： 度小满金融（原百度金融），北京海淀区，数据挖掘工程师。
 2017.06-2017.09： 百度，北京海淀区，大数据方向实习生。
  教育背景  2015.09-2018.07： 苏州大学，硕士，计算机技术（排名Top 15%）。
 2011.09-2015.07： 东南大学成贤学院，本科，计算机科学与技术（排名Top 3%）。 南京邮电大学，本科，计算机应用技术（排名Top 3%）。
  学校论文 毕业论文  2018年硕士毕业论文： 基于时空上下文共现的用户关系强度预测（ PDF ）</description>
    </item>
    
    <item>
      <title>机器学习笔记-决策树</title>
      <link>https://calxu.github.io/note/20200206_ml_decision_tree/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/note/20200206_ml_decision_tree/</guid>
      <description>在工业界，决策树（Decision Tree）是一类常见的机器学习方法。决策树综合可解释性、表征能力（线性模型 &amp;lt; 树模型 &amp;lt; 神经网络）、可操作性（调参）等优点，目前它是工业界应用最广的模型，很多基于树模型的开源的框架可供选择，比如Random Forest、GBDT、XGBoost、LightGBM 这些工业界最为常用的框架都是以决策树模型为基础的。
决策树是一种基本的分类与回归方法。这篇笔记主要介绍决策树的基本流程；其次介绍决策树的特征选择；再介绍决策树的剪枝；然后介绍一个历史上非常经典的决策树CART树；最后再介绍决策树模型的其它要点。
决策树的基本流程 决策树学习本质上是从训练数据集中归纳出一组分类规则。从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法（即贪心策略），近似求解这一最优化问题。这样得到的决策树是次最优的。
决策树的基本流程伪代码如下图所示（摘自《西瓜书》P74）。

决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回：
情形1： 当前结点包含的样本全属于同一类别，无需划分；
情形2： 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；
情形3： 当前结点包含的样本集合为空，不能划分；
在情形2中，我们把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别；在情形3中，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别。注意这两种情形的处理实质不同：情形2是在利用当前结点的后验分布，而情形3则是把父结点的样本分布为当前结点的先验分布。
上图是决策树学习基本算法的流程，基本算法流程中重点在第8行，如何选择最优的划分特征，所以决策树学习算法生成过程包括特征选择。
决策树特征选择 决策树学习基本算法的关键是第8行，即特征选择，也就是如何选择最优划分属性。一般而言，随着划分过程不断进行，我们希望决策的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高。
树结点分叉后“纯度”变化的数学描述通常有信息增益、信息增益比和基尼指数，下面对这三者分别进行介绍。
信息增益 在讲信息增益之前必须要提“信息熵”。&amp;rdquo;信息熵&amp;rdquo;（information entropy）是度量样本集合纯度最常用的一种指标，假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k (k=1,2,...,|Y|)$ ，则 $D$ 的信息熵定义为 $Ent(D)=-\sum_{k=1}^{|Y|} p_k \cdot log_2 p_k$ ， $Ent(D)$ 的值越小，则 $D$ 的纯度越高。
假定离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1, a^2, ..., a^V\}$ ， 若使用 $a$ 来对样本集 $D$ 进行划分，则会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本记为 $D^v$ 。可以根据信息熵的定义计算出 $D^v$ 的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重 $|D^v|/|D|$ ，即样本数越多的分支结点的影响越大，于是可计算出可用属生 $a$ 对样本集 $D$ 进行划分所获得的“信息增益”（information gain），即 $Gain(D, a)=Ent(D)-\sum_{v=1}^{V} \frac{D^v}{D} Ent(D^v)$ 。</description>
    </item>
    
    <item>
      <title>机器学习笔记-集成学习Boosting</title>
      <link>https://calxu.github.io/note/20200202_ml_boosting/</link>
      <pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/note/20200202_ml_boosting/</guid>
      <description>根据基学习器的生成方式，目前集成学习方法主流分成两大类，即Boosting 和 Bagging。Boosting集成是每个基学习器存在强依赖关系，必须串行生成，目前比较有代表性的工业界框架有AdaBoost、GBDT、XGBoost 和 LightGBM，其中后三者工业界比较常用；Bagging集成是每个基学习器间不需要存在依赖关系、可并行化，其中比较具有代表性的是随机森林（即 Random Forest）。
这篇笔记主要记录集成学习Boosting的思想，关于Bagging的思想可参考上一篇笔记。这篇笔记首先记录Boosting的基本思路；其次详细记录经典且具有代表性的提升算法AdaBoost；然后记录基分类器为决策树的boosting集成树模型（boosting tree）；最后记录一种特殊的boosting集成方法即梯度提升（gradient boosting）。
Boosting方法的基本思路 Boosting方法基于的思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好，就是“三个臭皮匠顶个诸葛亮”的道理。在1990年，Schapire证明一个类强可学习与一个类弱可学习是等价的，即一个类是强可学习的充分必要条件是这个类是弱可学习的。
这样一来，如果已经发现了“弱学习算法”，那么能否将它提升为“强学习算法”。发现弱学习算法通常要比发现强学习算法容易得多，那么如何具体实施提升，便成为开发提升方法时所要解决的问题。经典的具有代表的提升方法是AdaBoost算法。
对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则（强分类器）容易得多。Boosting集成就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称基本分类器），然后组合这些弱分类器，构成一个强分类器。
AdaBoost AdaBoost（Adaptive Boosting）有自适应集成之意。对于AdaBoost而言，有两个重点：一是在每一轮如何改变训练数据的权值；二是如何将弱分类器组合成一个强分类器。关于第一个问题，AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值，这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注，于是，分类问题被一系列的弱分类器“分而治之”。至于第二个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法，具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率较大的弱分类器的权值，使其在表决中起较小的作用。AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一种算法里。
AdaBoost算法描述 假设给定一个二类分类的训练数据集 $T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$ 。其中，每个样本点由实例与标记组成。实例 $x_i \in X \subseteq R^n$ ，标记 $y_i \in Y=\{-1,+1\}$ ，$X$ 是实例空间，$Y$ 是标记集合。AdaBoost利用以下算法，从训练数据中学习一系列弱分类器，并将这些弱分类器线性组合成为一个强分类器。
AdaBoost算法描述 输入：训练集 $T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中 $x_i \in X \subseteq R^n$ ，标记 $y_i \in Y=\{-1,+1\}$ ；弱学习算法；
输出：最终分类器 $G(x)$；
(1) 初始化训练数据的权值分布 $D_1=\{w_{11},...,w_{1i},...,w_{1N}\}, w_{1i}=\frac{1}{N}, i=1,2,...,N$ 。
(2) 对 $m=1,2,...,M$
　(a) 使用具有权值分布 $D_{m}$ 的训练数据集学习，得到基本分类器 $G_m(x):X \rightarrow \{-1,+1\}$ ；
　(b) 计算 $G_m(x)$ 在训练数据集上的分类误差率 $e_m=P(G_m(x_i) \neq y_i)=\sum_{i=1}^{N}w_{mi}I(G_m(x_i) \neq y_i)$ ，其中 $I(x)$ 为指示函数；</description>
    </item>
    
    <item>
      <title>机器学习笔记-集成学习Bagging</title>
      <link>https://calxu.github.io/note/20200118_ml_bagging/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/note/20200118_ml_bagging/</guid>
      <description>工作也近一年半的时间，工作这段时间一直聚焦于业务，会调参懂业务基本可以解决工作中绝大部分业务问题，很多时候不需要深入理解理论。 最近闲下来有时间把机器学习的理论知识重新梳理学习下。 这篇笔记学习下机器学习中集成学习的一些知识。
集成学习（ensemble learning）是很多机器学习框架所应用到的一个策略。 目前比较有代表性的集成方式是Boosting 和 Bagging。 Boosting集成是每个基学习器间存在强依赖关系、必须串行生成，目前比较有代表性的工业界框架有AdaBoost、GBDT、XGBoost和LightGBM，其中后三者工业界比较常用；Bagging集成是每个基学习器间不需要存在依赖关系、可并行化，其中比较具有代表性的是随机森林（即 Random Forest），工业也比较常用。
这篇笔记主要基于周志华老师的《机器学习》西瓜书集成学习部分，聚焦在集成学习之Bagging方法。首先记录有放回采样的过程和特性；其次介绍集成学习Bagging方法，Bagging集成是建立在有放回采样的基础之上；最后介绍随机森林，随机森林是典型的基于Bagging集成的一个扩展变体。
有放回采样（bootstrap sampling） 有放回采样（bootstrap sampling）周志华的西瓜书里也把它称为“自助法”。给定包含 $m$ 个样本的数据集 $D$，我们对它进行采样产生数据集 $D&#39;$。 过程描述如下：每次随机从 $D$ 中挑选一个样本，将其拷贝放入 $D&#39;$ 中，然后再将该样本放回初始数据集 $D$ 中，该样本在下次采样时仍有可能被采到；这个过程重复执行 $m$ 次，就可得到包含 $m$ 个样本的数据集 $D&#39;$，以上便是有放回采样的结果。 $D$ 中有一部分样本会在 $D&#39;$ 中多次出现，而另一部分样本不出现。 样本在 $m$ 次采样中始终不被采到的概率是 $(1-\frac{1}{m})^m$ ，取极限得到 $\lim_{m\to +\infty}(1-\frac{1}{m})^m = \frac{1}{e} \approx 0.368$ ，即通过有放回采样，初始数据集中 $D$ 约有36.8% 的样本未出现在采样数据集 $D&#39;$ 中，而那部分样本可作测试集。
有放回采样在数据集较小，难以有效划分训练/测试集时很有用；同时，有放回采样可以从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。
集成学习Bagging 欲得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然个体学习器完全独立在现实任务中无法做到，但可以设法使基学习器尽可能具有较大的差异。 给定一个训练数据集，进行反复采样，可产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器。 同时，为获得很好的集成，个体学习器不能太差，如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，不足以有效学习。 所以反复地有放回采样（bootstrap sampling）是一种有效的方式，同时可产生相互有交叠的采样子集。
Bagging（Bootstrap AGGregatING）集成学习中采样即是采用有放回采样的方式进行采样的。 采样过程描述如下：给定包含 $m$ 个样本的数据集，我们先随机取出一个样本放入采集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过 $m$ 次随机采样操作，我们得到含 $m$ 个样本的采样集。由有放回采样的性质得：初始训练集中约有63.2% (1-32.</description>
    </item>
    
    <item>
      <title>个人博客搭建与运维笔记</title>
      <link>https://calxu.github.io/note/20200107_personal_blog/</link>
      <pubDate>Tue, 07 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/note/20200107_personal_blog/</guid>
      <description>以前一般在CSDN上写博客，但现在CSDN广告越来越泛滥，无法忍受了。稍微研究了下目前可以写博客的平台 CSDN、简书、知乎甚至 微信里写文章，在看文章时都会受各种广告推荐的影响，记得有次在微信全神看推荐算法类的文章，突然中间给我插了条相亲的广告，当时非常恼火。
后来想了想各种博客平台为了赢利终将会采用各种广告的手段，所以还是自己搭建博客吧，而且对于一个做技术的人来说也确实有必要搭一个个人博客。下面就对我个人所面临的 框架选择、基础搭建过程、主题选择、部署和日常维护 做一些记录。
博客框架的选择 对于一个非专业Web开发的人来说，自己实施搭建成本太高了。所以一定要借用现有的开源框架来搭建，网上查了下主流框架主要有Jekyll、Hexo和Hugo，也看了很多人的blog，印象特深的是 谢益辉 ，他也讲述了他用Jekyll的一些坑，然后转向Hugo框架，我觉得他的个人博客非常的nice，所以也顺着他的路用Hugo来搭建了。至于Hexo和Hugo的比较，目前来看基于Hexo还是比基于Hugo搭建的人要多，教程也是Hexo居多。但是Hugo目前也已经渐渐形成生态，基于Hugo搭建的主题也非常丰富 [6] ，Hugo也是以速度性能见长，所以当时也就基于Hugo来搭建个人博客了。
Hugo是由Go语言实现的静态网站生成器，还是比较简单容易上手的。官方提供的quick start [2] 也是可以在短时间内快速搭建一个个人网站的。
对于这篇blog而言，主要是Hugo框架学习和操作实践、主题选择方面的一些注意点和部署方面的实践。关于Hugo的使用Google下，也有很多的教程，我比较推荐官网的 quick start [2] 、 两个Youtube视频 [8] [9] 和 参考文献中的教程 [4] [5] [7]  。
基础搭建过程 这里主要介绍在应用主题前，一些基础环境的搭建。与官网提供的 quick start 教程 [2] 一致，这里对生成的目录做了稍微详细的讲解。
安装Hugo环境 $ brew install hugo $ hugo version  以上通过 hugo version 来验证hugo环境安装是否成功。这里是基于macOS环境安装，其它操作系统参考 官网安装 。
生成个人网站 $ hugo new site blog_test  以上命令行会创建一个新的文件夹blog_test，该文件夹下包含Hugo最基本目录结构，其中blog_test目录下文件如下：
. ├ archetypes/ ├ config.toml ├ content/ ├ data/ ├ layouts/ ├ static/ └ themes/  生成文件包含6个文件夹 和 1个全局配置文件，下面依次对这7个文件做相应的说明。</description>
    </item>
    
    <item>
      <title>Links</title>
      <link>https://calxu.github.io/links/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/links/</guid>
      <description> 以下是个人收藏的一些值得自己学习的资料、平台和blog。
学习资料  周志华：《机器学习》西瓜书PDF
 李　航：《统计学习方法》第2版PDF
  学习平台  极客时间：高质量学习技术的平台  技术博客（偏工程）  王垠：思想值得学习；
 陈皓：曾阿里专家，每周都要逛他的推荐文章；
 赖明星：腾讯工程师；
 阮一峰：经常需逛逛他的blog；
 谢益辉：blog主题是follow他的，R语言高手；
 戴铭：IOS高手；
 antirez：redis创始人，提高了我对开源的认知；
 苏大老师：苏大计算机系挺喜欢的一位老师；
 IT草根：东大学长
 Byvoid：前Google工程师，技术大牛；
 张驰原：谷歌大脑工作；
  技术博客（偏学术）  Andrew Ng：偶尔会去看看吴老师最近的研究；
 吴军博士：非常喜欢吴军老师《浪潮之巅》这本书，但blog好像不怎么更新了；
 陈立杰：计算机理论，偶尔会关注计算机理论的发展；
 李航：李航老师，《统计学习方法》作者；
  其它博客  方舟子：观点值得学习；  </description>
    </item>
    
    <item>
      <title>利用开源ss工具快速搭建翻墙VPN</title>
      <link>https://calxu.github.io/note/20191101_shadowsocks/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/note/20191101_shadowsocks/</guid>
      <description>搭建ss的方法，网上有很多，请自行Google。 这里提供下链接ss官网的搭建方法， 目前GFW已经具备识别ss流量的能力，但每次好像都是国家大事件时才进行封堵，但封堵时也误杀了很多正常的流量的ip。 所以过了这段关键时期，GFW会放行，这就需要重新搭建ss。
这篇文章主要记录下搭建ss翻墙软件的最小配置，方便自己在GFW封杀后再快速搭建翻墙的VPN。
客户端配置 客户端软件下载  mac下载
 windows下载
   注：下载最新的Assets即可  客户端配置 这里提供下我的ss帐号密码供大家使用，别薅我流量，黑客也不要攻击我机器，VPN仅供程序员学习使用，不能作恶； ip可能会被GFW封，被封后我再重新切换下ip，关注这个blog即可。 该VPN已在2月初被封，目前海外阿里云服务器已能检测到ss代理的使用。
地址： 149.129.51.62:8432 加密方法： aes-256-cfb 密码： Dont_be_evil 备注： Dont_be_evil  客户端的工作原理（MAC版）： 启动软件会启动3个子程序，可通过 &amp;ldquo;ps aux | grep -i shadowsocks&amp;rdquo; 查看到 ShadowsocksX-NG、ss-local 和 privoxy 三个子程序； 与此同时，MAC中网络&amp;gt;高级&amp;gt;代理&amp;gt;自动代理配置 会被配置为软件自带的pac文件；
 ShadowsocksX-NG 只是一个Swift写的一个GUI壳子；
 ss-local为编译好了的程序，ss-local默认监听1080端口（可设置），接收socks协议的请求并转发至服务端；
 privoxy是对http协议的代理，监听1087端口，然后转发到1080端口由ss-local程序负责转发；
  注：以上原理详情可参考Github ShadowsocksX-NG官网
服务端配置 购买VPS  这里我购买的是阿里云香港区，供参考；  配置server端环境 安装环境： $ apt install python3-pip $ pip3 install shadowsocks  配置文件： $ cat /etc/shadowsocks.</description>
    </item>
    
    <item>
      <title>Git命令学习笔记02</title>
      <link>https://calxu.github.io/note/20190810_git/</link>
      <pubDate>Sat, 10 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/note/20190810_git/</guid>
      <description>目前使用Git管理项目半年时间，对Git最常用的基础命令已经全部掌握， 这里记录一些Git管理项目中一些常用命令参数，不了解这些参数也能应付大多数项目管理， 但了解这些参数会有利于代码的管理。
配置Git 初始设置 git --version # 查看git是否安装成功 git config --global user.name &amp;quot;Calvin,Xu&amp;quot; # 设置用户名 git config --global user.email &amp;quot;csxucaixu@gmail.com&amp;quot; # 设置邮箱 git config --global --list # 显示全局config的配置 git config --global color.ui auto # 提高git的可读性  需设置使用Git时的姓名和邮箱地址，以及提高Git的可读性，以上命令会在&amp;rdquo;~/.gitconfig&amp;rdquo;添加相应的设置:
[user] name = Calvin,Xu email = csxucaixu@gmail.com [color] ui = auto  自定义.gitignore .gitignore # git管理需忽略的文件，.gitignore配置的规则与linux通配符一致  常用命令参数 git add --help # 查看git add命令的使用文档，所有命令均可通过此方式查看使用说明  git add -u # 把本地有改动的文件添加至暂存区  git diff # 查看工作区与暂存区所含文件的区别 git diff filename # 查看工作区与暂存区指定文件的区别 git diff --cached # 查看暂存区与HEAD所含文件的区别 git diff --cached filename # 查看暂存区与HEAD指定文件的区别 git diff HEAD # 查看工作区与HEAD所含文件的区别 git diff HEAD filename # 查看工作区与HEAD指定文件的区别  git reset HEAD filename # 取消暂存区指定文件的修改，保持与最新分支一致 git checkout -- filename # 取消工作区指定文件的修改，保持与暂存区一致 git reset --hard commidID # HEAD恢复成commidID指向的状态，并且工作区和暂存区恢复成commitID指向的状态  git stash list # 列出存盘中的所有信息 git stash # 当前状态存盘 git stash pop # 存盘状态弹出  日志管理命令参数 git log # 显示提交的日志记录 git log --oneline # 以简洁的方式显示提交的日志记录 git log -n 4 --oneline # 以简洁的方式显示最近提交的4条日志记录 git log --all # 显示所有分支的提交日志记录 git log --all --online --graph # 以图形化的方式显示所有分支的日志记录  分支管理命令参数 git branch -a # 查看分支，-a会列出所有分支 git branch -v # 查看分支，-v参数会列出分支的描述信息  git merge dev # 将dev分支合并至当前分支 git merge --allow-unrelated-histories dev # 将不相关的dev分支合并至当前分支 git merge --abort # 恢复merge之前的状态  远程仓库命令参数 git remote # 列出已经存在的远程分支 git remote -v # 列出已经存在的远程分支详细信息 git remote rename # 修改某个远程仓库的简短名称 git remote add [shortname] [url] # 指定一个简单的名字来引用远程仓库  git clone origin local # 将远程origin仓库克隆至本地仓库  git fetch origin master # 将远程origin最新内容拉至本地master分支  git push origin master # 参数：git push &amp;lt;远程主机名&amp;gt; &amp;lt;远程分支名&amp;gt;，首次推送需加参数-u git push origin --tags # git push不会推送标签，需手动推送 git push origin --all # 推送本地所有分支  快捷操作 git mv # 给文件重命名，不通过linux中的mv命令来操作 git rm # 删除文件，不通过linux中的rm命令来操作  参考文献  [1] 极客时间.</description>
    </item>
    
    <item>
      <title>Git命令学习笔记01</title>
      <link>https://calxu.github.io/note/20190208_git/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://calxu.github.io/note/20190208_git/</guid>
      <description>工作半年时间了，做了一些项目，对代码的组织有点凌乱。 Git这么好的代码管理工具一直都养成不了使用的良好习惯，每次到代码一团乱偶尔也会误删，误删后才想起Git，这次狠下心决定系统学习下Git，以后坚持用Git。 这里作了一些自己学习的Git常用命令笔记，方便以后自己参考。 附下适合学习的LearnGitBranching可视化教程，以上资料供学习参考。
配置Git Linux安装Git sudo apt install git  安装完后需要配置Git。因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：名字和Email地址； Note：git config命令的 &amp;ndash;global 参数表示这台机器的所有Git仓库都会使用这个配置。
git config --global user.name &amp;quot;Calvin,Xu&amp;quot; git config --global user.email &amp;quot;csxucaixu@gmail.com&amp;quot;  自定义Git .gitignore # 忽略某些文件时，需要按格式编写.gitignore  常用命令 git init # 初始化Git仓库 git add # 把文件添加到暂存库 git commit -m # 把暂存库中文件提交到仓库，-m后面输入本次提交的概述 git status # 随时查看仓库当前的状态 git diff # 查看文件修改前后的difference git reset # 作用1：回退版本；作用2：把暂存区的修改撤消掉（即unstage） git reflog # 查看命令历史 git checkout # 作用1：丢弃工作区的修改（通常后接--转义）；作用2：切换分支 git rm # 删除工作区文件，并将此次删除放入暂存区；  git diff # 查看文件修改前后的difference，该命令可以查看工作区、暂存区、最新提交之间的差别。不加参数默 认查看工作区和暂存区的差别；后通常接HEAD（指向当前分支中最新一次提交的指针），在执行git commit命令之前通常先 执行git diff HEAD命令，比较本次提交与上次提交之前的差别。  git reset --hard # 回溯版本，后接版本号。比如HEAD，即表示当前版本；HEAD^表示上一个版本 git reset HEAD &amp;lt;file&amp;gt; # 可以把暂存区的修改撤消  git checkout -- &amp;lt;file&amp;gt; # 丢弃工作区的修改，回到最近一次git commit或git add的状态，‘--’作用是表示后面接的是 文件路径，以避免歧义，没有‘--’就变成了“切换到另一个分支“的命令。  git rm # 先手动删除文件，然后使用git rm &amp;lt;file&amp;gt;和git add &amp;lt;file&amp;gt;效果是一样的。  日志管理 git log # 显示从最近至最远的commit的版本 git commit --amend # 修改提交信息 git rebase -i # 修改历史commit；压缩历史commit (内容比较多，后续需继续学习)  git log # 显示从最近至最远的commit的版本。后可加上目录名，便会显示该目录下的日志；如果加的是文件名， 会显示与该文件相关的日志。常用参数：--pretty=oneline 精简显示；--graph 以图的方式显示；--abbrev-commit 显示 精简的commit id号；-p 参数显示文件前后差别。  分支管理 git branch # 查看分支，该命令会列出所有分支，当前分支前会标一个*号 git branch dev # 创建名为dev的新分支 git branch -a # 查看所有分支（包括本地分支和远程分支）的信息 git branch -d # 删除分支 git branch -D # 强制删除分支 git branch --set-upstream-to=origin/dev dev # 指定本地dev分支与远程origin/dev分支的链接  git checkout # 切换分支 git checkout -b # 创建并切换分支，等价于先git branch；再git checkout git checkout -b dev origin/dev # 创建远程origin的dev分支到本地dev分支 git checkout --orphan dev # 创建空白分支，完全干净的分支（不依赖任何父节点）  git merge # 合并某分支至当前分支 git merge --no-ff # 禁用默认的Fast Forward模式，在merge时生成一个新的commit  远程仓库 git remote # 查看远程仓库的信息 git remote -v # 显示更详细的信息 git remote add name url # 关联远程仓库  git clone # 克隆仓库 git fetch # 将远程主机最新内容拉至本地，用户需检查是否需合并至本机分支 git pull origin master # 将远程主机最新内容拉至本地后直接合并，git pull = git fetch + git merge git push origin master # 推送本地master分支内容至远程，首次推送需加-u参数以便与远程分支建立关联 git push --set-upstream origin dev # 若本地首次新建的dev分支，则新建分支需与远程分支进行关联  标签管理 git tag # 查看所有标签 git tag &amp;lt;tagname&amp;gt; # 用于新建一个标签，默认为HEAD，也可以指定commit id git tag -d # 删除一个本地标签 git tag -a &amp;lt;tagname&amp;gt; -m # 可以指定标签信息 git show &amp;lt;tagname&amp;gt; # 查看详细说明  快捷操作 git commit -am # 可以省略使用git add命令将已跟踪的文件放到暂存区。等价于git add ; git commit -m  参考文献  [1] 廖雪峰.</description>
    </item>
    
  </channel>
</rss>