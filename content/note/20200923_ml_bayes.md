---
title: '机器学习笔记-朴素贝叶斯法'
date: '2020-09-23'
categories:
  - Machine Learning
tags:
  - Machine Learning
---


最近在恶补统计学习方面的理论知识，结合《统计学习方法》和知乎一篇文章对朴素贝叶斯分类进行一次学习总结，方便自己后期复习。贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。**朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法**。

在看完《统计学习方法》书时，书上的数学公式整体上比较抽象，看得有点晦涩难懂。**最好的方式还是先看书上例子或知乎上的例子，然后通过例子再理解数学公式。** 



## 知识点疏理

**最核心的数学公式（即贝叶斯公式）：** `$P(X,Y)=P(X) \cdot P(Y|X) = P(Y) \cdot P(X|Y)$` 

**化简下得：** `$P(Y|X) = \frac{P(Y) \cdot P(X|Y)}{P(X)}$` 

换种形式表述上式： `$P(类别|特征) = \frac{P(类别) \cdot P(特征|类别)}{P(特征)}$`

上式中 `$P(类别)$` 是可以直接在训练集上求得，若将 `$P(特征)$`、`$P(特征|类别)$` 看成在类确定的条件下特征是条件独立的， `$P(特征)$`、`$P(特征|类别)$` 也可在训练集上容易求得。在求得等式右边后，就可以知道对应特征所属类别 `$P(类别|特征)$` 的概率了。



**计算两个注意点：**

1. `$P(特征|类别) = P(特征1, 特征2, ...特征n|类别) = P(特征1|类别) \cdot P(特征2|类别) \cdot ... \cdot P(特征n|类别)$` ，只有当在类确定的条件下特征都是条件独立的，上式才成立。

2. `$P(特征)$` 的计算应该用贝叶斯全概率公式（看书这里一开始很费解），**而非 `$P(特征) \neq P(特征1) \cdot P(特征2) \cdot ... \cdot P(特征n)$`** ，因为这里是前提假设是 **特征条件独立而非特征独立，所以应该用贝叶斯全概率公式**，即 `$P(特征) = \underset{所有类别}{\sum} ( {P(类别) \cdot \underset {所有特征}{\prod}P(特征|类别)  } )$` 。这里 `$P(特征)$` 是个常数，所以在《统计学习方法》中将它省略而不影响最终类别的判断。

特征独立也是朴素贝叶斯分类 **朴素** 一词的来源，朴素贝叶斯算法就是假设在类确定的条件下特征之间相互独立。



**为什么需要假设特征条件之间相互独立呢？**  第一个原因：若独立只需算 `$S(特征1) + S(特征2) + ... + S(特征n)$` 累加和的概率个数；若不独立，需算 `$S(特征1) \cdot S(特征2) \cdot ... \cdot S(特征n)$` 累乘积的概率个数，随着特征个数增加，复杂度是次方级增加；第二个原因：由于数据的稀疏性，很容易统计到0的情况。基于以上两个原因，朴素贝叶斯分类前提要求是特征之间相互独立，由于这是一个非常强的假设，所以会使得朴素贝叶斯变得简单，同时也会牺牲一定的分类准确率；**但在工业界，直接使用朴素贝叶斯方法的，也非常之少。个人认为根本原因也是前提假设特征相互独立，但事实情况是特征之间很多时候是存在内在关联关系的，这样会导致机器学习学不完整，所以在工业界更多是在使用决策树模型或神经网络模型。**  

`$P(类别)$` 、`$P(特征|类别)$` 、`$P(特征)$` 求解的过程就称为朴素贝叶斯法的参数估计。更进一步为避免这三类概率值求出来为0的情况，引入了一个 `$\lambda$` 项，引入 `$\lambda$` 后求解的过程称为贝叶斯估计。



## 总结

朴素贝叶斯的优点：（1）算法逻辑简单，易于实现；（2）分类过程中时空开销小。缺点：朴素贝叶斯模型假设特征之间相互独立，这个假设在实际应用中往往不成立，在特征个数比较多或特征之间相关性较大时，分类效果不好。



## 参考文献

1. [带你理解朴素贝叶斯分类算法](https://zhuanlan.zhihu.com/p/26262151)，知乎

2. 《统计学习方法》，李航
